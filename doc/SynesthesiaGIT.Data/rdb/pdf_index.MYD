	%2rcussive Beat tracking using real-time median filteringAndrew Robertson1, Adam Stark2, and Matthew E. P. Davies31 Centre for Digital Music, Queen Mary University of London, UK 2 Codasign Ltd, UK3 INESC TEC, PortugalAbstract. We present an efficient method for extracting the percussive component of a signal in real-time and a method for using this informa- tion to perform reliable beat tracking. The percussive signal can be used for the transcription of drum events and to learn the drum pattern for a song. We evaluate this method on several databases and thereby provide a comparison to other real-time methods.1 IntroductionBeat tracking algorithms aim to replicate the human ability to tap in time with music. The problem has been approached using event detection [6], multiple agent hypotheses [3], comb filter resonators [7] and autocorrela- tion [2]. As input to the beat tracking algorithm, most methods use an onset detection function [1] â€“ a mid-level representation that reflects the extent to which musical â€˜noveltyâ€™ occurs in the current frame.Since drums tend to drive the rhythmic pulse of a song, it makes sense to aim to synchronise most closely with the percussive elements of the music. Fitzgerald [5] applied median filtering to a spectrogram to separate the percussive and harmonic components of a signal. Whilst the technique is relatively simple to implement compared to other source separation methods, in a real-time system it can be computationally expensive and introduce a significant latency. In this paper, we present a method for percussive separation that can be carried out in real-time and evaluate a percussive detection function to improve real-time beat tracking. Our intended application is live synchronisation of audio and video.2 MethodFitzgeraldâ€™s [5] technique for percussive separation is motivated by the observation that percussive features tend to consist of wide band noise across all frequencies and appear as vertical lines in the spectrogram,In MML 2013: International Workshop on Machine Learning and Music, ECML/PKDD, Prague, 2013.ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼Original order Sortedremovedto insert0.320.531.691.230.210.570.700.810.620.73ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼0.210.320.530.570.620.700.811.231.69ï¿¼ï¿¼ï¿¼ï¿¼0.32ï¿¼ï¿¼ï¿¼0.53ï¿¼ï¿¼1.69ï¿¼1.23ï¿¼0.21ï¿¼0.57ï¿¼0.70ï¿¼0.81ï¿¼0.62ï¿¼0.73ï¿¼0.21ï¿¼ï¿¼0.53ï¿¼0.57ï¿¼0.62ï¿¼0.70ï¿¼ï¿¼0.81ï¿¼1.23ï¿¼1.69ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼0.73Fig. 1. Illustration showing how we add a new value (0.73) and efficiently find the median of the most recent N values.whilst harmonic components consist of frequencies which persist, and thus appear as horizontal lines. Median filtering is applied in both the frequency and time directions to create separate percussive-enhanced and harmonic-enhanced spectrograms. These can be compared to create a per- cussive mask which indicates the extent to which the energy of each bin belongs to the percussive component. A percussive detection function is obtained by summing the percussive component at a given time frame across all frequencies. By summing frequency bins below 120Hz, and fre- quency bins between 200 Hz and 500 Hz, we can create functions which correlate to kick and snare strength respectively.To carry out percussive separation in real-time, we require an efficient technique for median filtering. Median filters operate by replacing a given value in a signal with the median of the set of N values centred on the value under consideration. By storing the sorted set of values and the sequential set of values (preserving the order in which they arrived), we can simplify the calculation. Figure 1 illustrates how the process works in practice. Given a new value, we need to remove the oldest value, determined using the sequential array, by searching for this value in the sorted set of values and removing it from both sets. Finding the median then only requires adding the new value to the sequential set (at the most recent position) and inserting the new value at the correct location into the sorted set.For a standard FFT of framesize 2048, we might limit the median fil- tering, but this still require several hundred values. Our proposed method allows us to calculate a thousand median filter values in approximately 0.7 msec rather than 2.3 msec. When processing audio in real-time, this is a significant reduction in the time taken for median filtering.We use this percussive detection function as the input to our beat tracking algorithm. We adapted the method from Stark et al. [8], which uses a tempo following technique from Davies and Plumbley [2] and dy-ï¿¼ï¿¼1 0.8 0.6 0.4 0.2 01 0.8 0.6 0.4 0.21 0.8 0.6 0.4 0.2 01 0.8 0.6 0.4 0.210001000100010001100 1200 13001100 1200 13001100 1200 13001100 1200 1300Detection Function Framesï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼Fig. 2. The top figure shows the percussive detection function (solid) and the complex spectral difference onset detection function (dotted). The second and third figures show kick and snare strength functions with detected onsets indicated as vertical lines. The bottom plot shows the cumulative detection function (solid line) with the Gaussian window (dotted line) used when making beat predictions (dotted vertical lines).namic programming method from Ellis [4]. Initialisation is carried by key commands. A synchronisation process adjusts for the latency inherent due to percussive separation and uses linear regression to predict the subsequent beat times. By quantising the kick and snare detection func- tions relative to the beat positions, we can learn a representation of the drum pattern. The inter-beat interval is divided into twelve equidistant temporal bins to include both triplets and sixteenth note events.3 EvaluationWe evaluated the real-time C++ implementation relative to the Btrack al- gorithm [8]. Since our intended use is in live performance, our beat tracker requires initialisation. This seems a preferable design choice rather than introducing instability by having the beat tracker potentially switch to alternative tempo hypotheses. For the evaluation, we set the beat tracker to the correct beat and phase at five seconds into the song. Across the database, the beat tracker largely outperforms the Btrack algorithm, al- though providing initialisation from the ground truth inevitably intro- duces a bias in favour our proposed method.We also created a Max for Live device that enables the beat tracker to control Ableton Live. A video of the the system is viewable at http://youtu.be/fT8DXecuXhg and illustrates the systemâ€™s response to tempo fluctuations.Table 1. Comparison between Stark et al.â€™s Btrack algorithm and our method. The input feature for Btrack is the complex spectral difference onset detection function.4 ConclusionsIn this paper, we have presented a method for median filtering that en- ables percussive separation in real-time. We perform beat tracking using a percussive detection function and generate functions that correlate with kick and snare events. By learning the patterns of these drum events, we can form expectations for the drum patterns played in a bar.References1. J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M. Davies, and M. Sandler. A tutorial on onset detection in music signals. IEEE Transactions on Speech and Audio Processing, 13(5, Part 2):1035â€“1047, 2005.2. M. E. P. Davies and M. D. Plumbley. Context-dependent beat tracking of musical audio. IEEE Transactions on Audio, Speech and Language Processing, 15(3):1009â€“ 1020, 2007.3. S. Dixon. Automatic extraction of tempo and beat from expressive performances. Journal of New Music Research, 30:39â€“58, 2001.4. D. P. W. Ellis. Beat tracking by dynamic programming. Journal of New Music Research, 36(1):51â€“60, 2007.5. D. Fitzgerald. Harmonic/percussive separation using median filtering. In 13th Int. Conference on Digital Audio Effects (DAFx-10), Graz, Austria, 2010.6. M. Goto and Y. Muraoka. A beat tracking system for acoustic signals of music. In Proceedings of the Second ACM International Conference on Multimedia, pages 365â€“372, 1994.7. E. D. Scheirer. Tempo and beat analysis of acoustic musical signals. The Journal of the Acoustical Society of America, 103(1):588â€“60, 1998.8. A. M. Stark, M. E. P. Davies, and M. D. Plumbley. Real-time beat-synchronous analysis of musical audio. In Proc. DAFX-09, pages 299â€“304, 2009.ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼Measureï¿¼Databaseï¿¼Methodï¿¼ï¿¼cmlCï¿¼cmlTï¿¼amlCï¿¼amlTï¿¼Beatles (179)ï¿¼ï¿¼Btrack Proposedï¿¼ï¿¼ï¿¼48.3 63.2ï¿¼ï¿¼ï¿¼63.5 78.3ï¿¼ï¿¼56.3 63.2ï¿¼ï¿¼75.9 78.3ï¿¼ï¿¼Rock Corpus (200)ï¿¼ï¿¼Btrack Proposedï¿¼ï¿¼ï¿¼42.8 56.9ï¿¼ï¿¼ï¿¼58.1 75.5ï¿¼ï¿¼54.4 57.5ï¿¼ï¿¼75.7 76.6ï¿¼ï¿¼Klapuri (474)ï¿¼ï¿¼Btrack Proposedï¿¼ï¿¼ï¿¼48.4 60.8ï¿¼ï¿¼ï¿¼57.0 70.5ï¿¼ï¿¼56.0 60.8ï¿¼ï¿¼68.7 70.5ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼,  internal-pdf://1350490027/MLMU_Robertson.pdf               #      %¼ú4
         1967513926/Ellis07-bea )             1350490027/MLMU_Robertson.pdf%  Pe%b       %tú4   áttrack.pdf¡  Beat Tracking by Dynamic ProgrammingDaniel P.W. EllisLabROSA, Columbia University, New York July 16, 2007AbstractBeat tracking â€“ i.e. deriving from a music audio signal a sequence of beat instants that might correspond to when a human listener would tap his foot â€“ involves satisfying two con- straints: On the one hand, the selected instants should generally correspond to moments in the audio where a beat is indicated, for instance by the onset of a note played by one of the instru- ments. On the other hand, the set of beats should reflect a locally-constant inter-beat-interval, since it is this regular spacing between beat times that defines musical rhythm. These dual constraints map neatly onto the two constraints optimized in dynamic programming, the local match, and the transition cost. We describe a beat tracking system which first estimates a global tempo, uses this tempo to construct a transition cost function, then uses dynamic programming to find the best-scoring set of beat times that reflect the tempo as well as corresponding to moments of high â€˜onset strengthâ€™ in a function derived from the audio. This very simple and computationally efficient procedure is shown to perform well on the MIREX-06 beat track- ing training data, achieving an average beat accuracy of just under 60% on the development data. We also examine the impact of the assumption of a fixed target tempo, and show that the system is typically able to track tempo changes in a range of Â±10% of the target tempo.11 IntroductionResearchers have been building and testing systems for tracking beat times in music for several decades, ranging from the â€˜foot tappingâ€™ systems of Desain and Honing [1999], which were driven by symbolically-encoded event times, to the more recent audio-driven systems as evaluated in the MIREX-06 Audio Beat Tracking evaluation [McKinney and Moelants, 2006a]; a more complete overview is given in the lead paper in this collection [McKinney et al., 2007].Here, we describe a system that was part of the latter evaluation, coming among the statistically- equivalent top-performers of the five systems evaluated. Our system casts beat tracking into a simple optimization framework by defining an objective function that seeks to maximize both the â€œonset strengthâ€ at every hypothesized beat time (where the onset strength function is derived from the music audio by some suitable mechanism), and the consistency of the inter-onset-interval with some pre-estimated constant tempo. (We note in passing that human perception of beat instants tends to smooth out inter-beat-intervals rather than adhering strictly to maxima in onset strength [Dixon et al., 2006], but this could be modeled as a subsequent, smoothing stage). Although the requirement of an a priori tempo is a weakness, the reward is a particularly efficient beat-tracking system that is guaranteed to find the set of beat times that optimizes the objective function, thanks to its ability to use the well-known dynamic programming algorithm [Bellman, 1957].The idea of using dynamic programming for beat tracking was proposed by Laroche [2003], where an onset function was compared to a predefined envelope spanning multiple beats that incorporated expectations concerning how a particular tempo is realized in terms of strong and weak beats; dynamic programming efficiently enforced continuity in both beat spacing and tempo. Peeters [2007] developed this idea, again allowing for tempo variation and matching of envelope patterns against templates. By contrast, the current system assumes a constant tempo which allows a much simpler formulation and realization, at the cost of a more limited scope of application.The rest of this paper is organized as follows: In section 2, we introduce the key idea of formulating beat tracking as the optimization of a recursively-calculable cost function. Section23 describes our implementation, including details of how we derived our onset strength function from the music audio waveform. Section 4 describes the results of applying this system to MIREX- 06 beat tracking evaluation data, for which human tapping data was available, and in section 5 we discuss various aspects of this system, including issues of varying tempo, and deciding whether or not any beat is present.2 The Dynamic Programming Formulation of Beat TrackingLet us start by assuming that we have a constant target tempo which is given in advance. The goal of a beat tracker is to generate a sequence of beat times that correspond both to perceived onsets in the audio signal at the same time as constituting a regular, rhythmic pattern in themselves. We can define a single objective function that combines both of these goals:NNC({ti})=/  internal-pdf://1967513926/Ellis07-beattrack.pdf